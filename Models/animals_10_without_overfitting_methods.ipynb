{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import InputLayer, Dense, Conv2D, MaxPooling2D, Flatten, Dropout, Rescaling #InputLayer, HiddenLayer, OutputLayer\n",
    "from tensorflow.keras.models import Sequential #Feed-Forward-NN\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import matplotlib.pyplot as plt #Plot Images\n",
    "import os #Access files and folders\n",
    "import numpy as np #Calculations\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(\"GPUs detected: \", len(physical_devices))\n",
    "if physical_devices:\n",
    "    print(\"GPU will be used\")\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) #Use GPU if available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"G:/Python/data/10_animals\" #Path to dataset\n",
    "height = 300; width = 300 #Rescale images in dataset to set size\n",
    "batch_size = 64 #determine batch size for training\n",
    "\n",
    "train = tf.keras.preprocessing.image_dataset_from_directory( #Loading training-data\n",
    "    path,\n",
    "    image_size=(height, width),\n",
    "    validation_split=0.25, #Determine how many images will be used for validation\n",
    "    subset=\"training\",\n",
    "    seed=123, #Shuffling and Transformation\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "val = tf.keras.preprocessing.image_dataset_from_directory( #Loading validation-data\n",
    "    path,\n",
    "    image_size=(height, width),\n",
    "    validation_split=0.25,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "classes = train.class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    InputLayer((height, width, 3)), #InputLayer\n",
    "    tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "\n",
    "    Conv2D(32, (3,3), padding='same', activation='relu'), #ConvolutionLayer\n",
    "    MaxPooling2D(), #PoolingLayer\n",
    "\n",
    "    Conv2D(64, (3,3), padding='same', activation='relu'), #ConvolutionLayer\n",
    "    MaxPooling2D(), #PoolingLayer\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'), #ConvolutionLayer\n",
    "    MaxPooling2D(), #PoolingLayer\n",
    "\n",
    "    Conv2D(128, (3,3), padding='same', activation='relu'), #ConvolutionLayer\n",
    "    MaxPooling2D(), #PoolingLayer\n",
    "\n",
    "    Flatten(), #FlattenLayer transforms input into 1-dimensional Array to be processed by Dense Layers\n",
    "    Dense(128, activation='relu'), #FullyConnectedLayer\n",
    "    Dense(64, activation='relu'), #FullyConnectedLayer\n",
    "    Dense(len(classes), activation='softmax') #OutputLayer\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['accuracy'] #tracking for accuracy\n",
    ")\n",
    "\n",
    "model.summary() #overview of built model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train, validation_data=val, epochs=20) #Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(os.path.join(\"C:/Users/maxis/Desktop/Proseminar - CNN/Models\",\"animals_10_(overfitting_cut)_59_val_accuracy.h5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = tf.keras.preprocessing.image.load_img(\"G:/Python/data/img_from_google/cat1.jpg\", target_size=(height, width)) #Load image thats not already in the dataset\n",
    "\n",
    "plt.imshow(img) #Plot image\n",
    "plt.show()\n",
    "\n",
    "img = tf.keras.preprocessing.image.img_to_array(img) #Convert image into array\n",
    "img = np.expand_dims(img, axis=0) #Add dimensions to array to be processed by the model\n",
    "\n",
    "prediction = model.predict(img) #Model predicts class of image\n",
    "\n",
    "print()\n",
    "\n",
    "prediction_probabilities = tf.math.top_k(prediction, k=5) #Get top-5 predictions\n",
    "top_5_scores = prediction_probabilities.values.numpy().tolist() #Get top-5 probabilities\n",
    "dict_class_entries = prediction_probabilities.indices.numpy().tolist() #Get matching classes to probabilities\n",
    "for label, confidence in zip(dict_class_entries[0], top_5_scores[0]):\n",
    "    print(f\"Image predicted to be {classes[label]} with confidence of {round(confidence*100, 3)}%\") #Printing the class and probability"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
